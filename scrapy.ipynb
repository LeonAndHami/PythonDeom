{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【scrapy的使用】\n",
    "\n",
    "\n",
    "创建scrapy 项目\n",
    "scrapy startproject project_name \n",
    "\n",
    "创建爬虫\n",
    "scrapy genspider spider_name 允许爬取的范围\n",
    "\n",
    "运行爬虫 (进入到爬虫所有目录)\n",
    "scrapy crawl spider_name\n",
    "\n",
    "\n",
    "使用pipeline\n",
    "1、实现pipelines.py中的 process_item 方法\n",
    "\n",
    "2、去配置文件中开启pipeline\n",
    "ITEM_PIPELINES = {\n",
    "    'demo01.pipelines.Demo01Pipeline': 300, #数值越大数先执行\n",
    "}\n",
    "\n",
    "\n",
    "使用logging\n",
    "去配置文件中配置\n",
    "\n",
    "LOG_LEVEL = \"WARNING\"\n",
    "LOG_FILE = \"./file.log\"\t#保存路径，设置后不会再在终端中显示日志，需要使用logger\n",
    "\n",
    "\n",
    "日志的级别有\n",
    "\n",
    "CRITICAL - 严重错误\n",
    "ERROR - 一般错误\n",
    "WARNING - 警告信息\n",
    "INFO - 一般信息\n",
    "DEBUG - 调试信息  默认\n",
    "\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.warning('your message') 输出格式: 2019-5-20 [root] WARNING:your message，不知道在哪个文件\n",
    "\n",
    "logger = loggin.getLogger(__name__)\n",
    "logger.warning('your message') 输出格式: 2019-5-20 [your file] WARNING:your message\n",
    "\n",
    "# logging模块的设置可以通过loggin.baseConfig()来完成，百度一下参数即可\n",
    "\n",
    "\n",
    "关于scrapy中实现翻页思路：\n",
    "1、找到下一页的地址\n",
    "2、构造一人下一页url地址的request请求传给调度器\n",
    "\n",
    "next_url = response.xpath('pettern')\n",
    "while len(next_url) > 0:\n",
    "    yield scrapy.Request(next_url,callback=self.parse) # yield request对象，就可以交给scrapy引擎\n",
    "\n",
    "\n",
    "\n",
    "【scrapy.Request知识点】\n",
    "scrapy.Request(url,[,callback,method=\"GET\",headers,body,cookie,meta,dont_filter=False])\n",
    "注：中括号中的参数是可选参数\n",
    "常用参数为：\n",
    "callback：指定传入的url交给哪个解析函数去处理\n",
    "\n",
    "meta：实现在不同的解析函数中传递数据，meta默认会携带部分信息，如下载延迟、请求深度等\n",
    "别的解析函数中取meta\n",
    "data = response.meta\n",
    "\n",
    "注意：一般在解释函数中，通过都会有for循环去遍历，提取数据给item赋值，如果用meta参数将item交给下一个解释函数继续处理，而后面的解释函数\n",
    "又有for循环时，很可能会导致item的数据错乱，或者重复，或者被覆盖了，因为scrapy的操作是【异步的】，后面解释函数在处理item时，前面解释函数\n",
    "也在使用item这个引用，还有可能将引用指向了一个新的内存地址，这种情况，用meta传递item，就要注意使用深拷贝【copy.deepcopy()】\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dont_filter：让scrapy的去重不会过滤当前的url,scrapy默认有url去重功能，对需要重复请求的url有重要用途\n",
    "\n",
    "注意点：Request已经提供了cookie参数，cookie就不要再放到headers中去了\n",
    "\n",
    "\n",
    "【pipeline】\n",
    "pipeline中写入数据到mongodb，记得将item转为字典\n",
    "\n",
    "\n",
    "【scrapy shell】\n",
    "scrapy shell 是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，也可以用来测试xpath\n",
    "\n",
    "用法：\n",
    "scrapy shell url\n",
    "\n",
    "response.url：当前响应的url地址\n",
    "response.request.url：当前响应的请求的url地址\n",
    "response.headers：响应头\n",
    "response.body：响应体，也就是html代码，默认是byte类型\n",
    "response.request.headers：当前响应的请求头\n",
    "\n",
    "\n",
    "【setting相关】\n",
    "\n",
    "settings中可以放一些全局变量供多个地方使用，一般全大写\n",
    "1、spider 对象里面有settings属性，可直接使用\n",
    "self.settings['key']\n",
    "self.settings.get('key','default_value')\n",
    "\n",
    "2、导入setting\n",
    "from your_project import settings\n",
    "print(settings.LOG_LEVEL)\n",
    "\n",
    "ROBOTSTXT_OBEY = True #是否遵循robots.txt中的规则\n",
    "\n",
    "CONCURRENT_REQUESTS # 并发数量\n",
    "\n",
    "DOWNLOAD_DELAY # 下载延迟\n",
    "\n",
    "COOKIES_ENABLED = True  # 请求是否带cookie\n",
    "\n",
    "COOKIES_DEBUG = True # 开启cookie调试，会在调试信息中看到cookie的信息\n",
    "\n",
    "\n",
    "【pipeline中的方法】\n",
    "\n",
    "    def __init__(self, mongo_url, db_name):\n",
    "        self.mongo_url = mongo_url\n",
    "        self.db_name = db_name\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(mongo_url=crawler.settings.get(\"MONGO_URL\"), db_name=crawler.settings.get(\"DB_NAME\"))\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = MongoClient(self.mongo_url)\n",
    "        self.db = self.client[self.db_name]\n",
    "        self.context = self.db[\"users\"]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.context.update({'url_token': item['url_token']}, dict(item), True)\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "【scrapy中的CrawlSpider】 练手地址 http://www.cbrc.gov.cn/chinese/newIndex.html\n",
    "简化：找查下一页url操作，构造自己的requests请求这一动作\n",
    "满足某个条件的url地址，才发送给引擎，同时能够指定callback函数。\n",
    "\n",
    "生成crawlspider的命令：\n",
    "scrapy genspider -t crawl spider_name example.com\n",
    "\n",
    "生成的爬虫代码基本一样的，但是spider的父类变成了 CrawlSpider ,多了一个 rules元组，少了parse方法\n",
    "rules = (\n",
    "    Rule(LinkExtractor(allow=r'正则'), callback='parse_item', follow=True),\n",
    ")\n",
    "LinkExtractor 链接提取器，提取url地址，即使url不完整，也不用管，框架会自动补全。\n",
    "callback 提取出来的url地址的 response 会交给callback处理，不需要callback可以为None\n",
    "follow 当前url地址的响应是否重新经过rules来提取url地址\n",
    "\n",
    "注意：不能定义parse方法，有特殊意义。\n",
    "\n",
    "\n",
    "\n",
    "【CrawlSpider补充】了解，需要用时回头看\n",
    "\n",
    "LinkExtractor 更多常见参数：\n",
    "\n",
    "allow：满足括号中‘正则表达式’的url会被提取，如果为人，则全部匹配。\n",
    "\n",
    "deny：满足括号中的‘正则表达式’的url 不会 被提取，优先级高于allow。\n",
    "\n",
    "allow_domains:会被提取的链接的domains。\n",
    "\n",
    "deny_domains：不会被提取链接的domains。\n",
    "\n",
    "restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接，即xpath满足范围内的url\n",
    "地址会被提取\n",
    "\n",
    "\n",
    "\n",
    "spider.Rule常见参数：\n",
    "\n",
    "link_extractor：是一个Link Extrack对象，用于定义需要提取的链接。\n",
    "\n",
    "callback：从link_extractor中每获取到链接时，参数所指定的值作为回调函数。\n",
    "\n",
    "follow：指定根据该规则从response提取的链接是否需要跟进，如果callback为None，\n",
    "follow默认设置为True，否则默认为False。\n",
    "\n",
    "process_links：指定该spider中哪个函数将会被调用，从link_extractor中获取到链接列表时\n",
    "将会调用此函数，该方法主要用来过滤url。\n",
    "\n",
    "process_request:指定该spider中哪个函数将会被调用，该规则提取到每个request时都会调用\n",
    "该函数，用来过滤request。\n",
    "\n",
    "\n",
    "\n",
    "【scrapy 模拟登录】\n",
    "a、直接携带cooke\n",
    "b、找到发送post请求的url地址，带信表单信息发送请求\n",
    "c、重写父类的 start_resquests方法，自己构建post请求，把scrapy.Request的method参数填上POST\n",
    "\n",
    "\n",
    "在start_requests方法中带cookie，不要放在headers中\n",
    "def start_requests(self):\n",
    "    cookies = {i.split(\"=\")[0]:i.split(\"=\")[1] for i in cookies_str.split(\"; \")}\n",
    "    yield  scrapy.Request(selft.start_urls[0],callback=callback,cookies=cooikis)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "【下载中间件】\n",
    "与pipeline本样，需要定义一个类(middleware文件中)，然后在settings中开启\n",
    "\n",
    "\n",
    "下载中件间的默认方法\n",
    "process_request(self,request,spider):\n",
    "    pass # 当每个request通过下载中间件时，该方法被调用\n",
    "    \n",
    "\n",
    "process_response(self,request,response,spider):\n",
    "    pass # 当下载器完成http请求，传递响应给引擎的时候调用。\n",
    "    # 这个方法一定要return\n",
    "    # return request  表示这个请求经过引擎会交给调度器\n",
    "    # return response 表示这个响应会经过引擎交给爬虫\n",
    "    \n",
    "\n",
    "下载中间件中，通过可以做反爬虫处理\n",
    "\n",
    "class RandomUserAgent(object):\n",
    "    def process_request(selff,request,spider):\n",
    "        USER_AGENT = spider.settings.get('key')\n",
    "        useragent = random.choice(USER_AGENT)\n",
    "        request.headers.['User-Agent'] = useragent\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class ProxyMiddleware(object):\n",
    "    def process_request(self,request,spider):\n",
    "        request.meta['proxy'] = \"ip:port\"  # 请求https的url，要用https的代理？\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "【scrapy post登录】\n",
    "1、scrapy.Request 方法 method改成POST\n",
    "\n",
    "2、scrapy.FromRequest 专门用来执行post提取的，可以带上表单、headers等参数\n",
    "\n",
    "\n",
    "def parse(self, response):\n",
    "    # 从start_urls 进来登录页面，可能需要提取一些tooken之类的表单参数\n",
    "    post_data = {\"account\":\"account\",\"pwd\":\"pwd\", ...} # 表单字典\n",
    "    yield scrapy.FromRequest(login_url,formdata=post_data,call_back=self.after_login)\n",
    "    \n",
    "    \n",
    "def after_login(selft,response):\n",
    "    pass\n",
    "    #post数据后的处理方法，不管登录是否成功\n",
    "\n",
    "\n",
    "\n",
    "scrapy还能帮我们自动寻找form表单提取的url地址，\n",
    "\n",
    "\n",
    "def parse(self,response):\n",
    "    formdata = {表单域的名:对应的值,...}\n",
    "    yield scrapy.FormRequest.from_response(response,formdata=formdata,callback=callback)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "demo:\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class GithubSpider(scrapy.Spider):\n",
    "    name='github'\n",
    "    allowed_domains = ['github.com']\n",
    "    start_urls = ['https:github.com/login']\n",
    "\n",
    "    def parse(self,response):\n",
    "    yield scrapy.FormRequest.from_response(\n",
    "        response, # 自动从response中寻找form表单\n",
    "        formdata={'login':'your_account','password':'your_passworld'},\n",
    "        callback=self.after_login\n",
    "    )\n",
    "\n",
    "    def after_login(self, response):\n",
    "        pass\n",
    "\n",
    "\n",
    "from_response 方法还有很多参数，可以传到定义看具体参数，方便scrapy准确找到正确的form表单\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "【scrapy redis】 配置\n",
    "\n",
    "SCHEDULER = \"scrapy_redis.scheduler.Scheduler\"\n",
    "DUPEFILTER_CLASS = \"scrapy_redis.dupefilter.RFPDupeFilter\"\n",
    "REDIS_URL = 'redis://192.168.3.8:6379' # 无密码\n",
    "\n",
    "# 有密码\n",
    "#REDIS_URL = 'redis://your_account:password@ip:6379'  \n",
    "\n",
    "# 每次启动都清空队列\n",
    "# SCHEDULER_FLUSH_ON_START =True\n",
    "\n",
    "# 是否在爬取完成之后清空队列\n",
    "# SCHEDULER_PERSIST = True\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
